* ymd_DL

Deep Learning Library

** Requirement
- [[https://github.com/yamada-github-account/ymd_util][ymd_util]]
- C++20 compiler for ymd_util/MNIST.hh
  - std::endian
- C++17 compiler
  - structured bindings
  - fold expression

** Simple Usage

See [[file:hello_MNIST.cc][hello_MNIST.cc]]

1. Include "DL_core.hh" (and other headers)
2. Create Network by ">>" operator
   - ~auto DL = ymd::InputLayer<double>{5} >> ymd::ReLU{10} >>
     ymd::SoftMax_CrossEntropy{5};~
3. Back Propagete by "<<" operator
   - ~DL << std::make_tuple(data,label)~
4. Update
   - ~DL.Update(eps,L1,L2)~


** Next Step

See [[file:hello_polynomial.cc][hello_polynomial.cc]]

- Spliting Train Test data ~ymd::split_train_test~
  - You can take return values by structured binding
    #+BEGIN_SRC C++
    auto [train_X,train_y,test_X,test_y] = ymd::split_train_test(X,y,0.7);
    #+END_SRC
- Adaptive gradient manager with suitable shape can be made by ~NeuralNet::MakeAdaptive~
- Shuffle data set and mini-batch operation can be achieved with range adaptors
  #+BEGIN_SRC C++
  for(auto&& Batch:
	ymd::zip(train_data,train_label) |
	ymd::adaptor::shuffle_view{} |
	ymd::adaptor::sliding_window{batch_size,batch_size}){
    for(auto&& DataLabel: Batch){ DL << DataLabel; }
    DL.Update(adam,L1,L2);
  }
  #+END_SRC


** Archtecture and Implement Techniques

*** Class Archtecture
[[file:class.png]]

=NeuralNet= class has NonOutputLayer and =OutputLayer=. Theses layer
classes are stored with ~std::tuple<...>~ and managed by static polymorphism.


=NonOutputLayer= class has =Layer= class and =WeightBias= class. Since the
back propagation of weights and biases require activated values as
well as propagated gradient from the next layer, These 2 classes are
managed together as =NonOutputLayer= class.

*** Static Polymorphism
In usual dynamic polymorphism, pointers to classes derived from a
interface class are managed with unified way. This can be achived by
virtual function, whose address is recorded in =vtable= in class
instances and refered every time the function is called. This dynamic
runtime method cannot be optimized by any compiler.

By using =std::tuple= (from C++11), parameter pack (from C++11),
generic labmda (from C++14), folding expression (from C++17), and so
on, different classes with same name members can be treated like
dynamic polymorphism.

** Note
- Multiple Back Propagation without Update sum up gradients.
- Weight Initialization
  - He Normalization (before ReLU)
  - Glorot Normalization (before Sigmoid, Identity_SquareError, and
    SoftMax_CrossEntropy)
- Only Adam is implemented as Adaptive Gradient Update
- No GPU acceleration


** Test Environment
- Mac OS X 10.13.6
- g++ (MacPorts gcc8 8.2.0_0) 8.2.0
- g++ -O3 -march=native -Wa,-q -fdiagnostics-color=auto -std=c++2a
