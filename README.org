* ymd_DL

Deep Learning Library

** Requirement
- [[https://github.com/yamada-github-account/ymd_util][ymd_util]]
- C++20 compiler for ymd_util/MNIST.hh
  - std::endian
- C++17 compiler
  - structured bindings
  - fold expression

** Simple Usage

See [[file:hello_MNIST.cc][hello_MNIST.cc]]

1. Include "DL_core.hh" (and other headers)
2. Create Network by ">>" operator
   - ~auto DL = ymd::InputLayer<double>{5} >> ymd::ReLU{10} >>
     ymd::SoftMax_CrossEntropy{5};~
3. Back Propagete by "<<" operator
   - ~DL << std::make_tuple(data,label)~
4. Update
   - ~DL.Update(eps,L1,L2)~


** Next Step

See [[file:hello_polynomial.cc][hello_polynomial.cc]]

- Spliting Train Test data ~ymd::split_train_test~
  - You can take return values by structured binding
    #+BEGIN_SRC C++
    auto [train_X,train_y,test_X,test_y] = ymd::split_train_test(X,y,0.7);
    #+END_SRC
- Adaptive gradient manager with suitable shape can be made by ~NeuralNet::MakeAdaptive~
- Shuffle data set and mini-batch operation can be achieved with range adaptors
  #+BEGIN_SRC C++
  for(auto&& Batch:
	ymd::zip(train_data,train_label) |
	ymd::adaptor::shuffle_view{} |
	ymd::adaptor::sliding_window{batch_size,batch_size}){
    for(auto&& DataLabel: Batch){ DL << DataLabel; }
    DL.Update(adam,L1,L2);
  }
  #+END_SRC

** Note
- Multiple Back Propagation without Update sum up gradients.
- Weight Initialization
  - He Normalization (before ReLU)
  - Glorot Normalization (before Sigmoid, Identity_SquareError, and
    SoftMax_CrossEntropy)
- Only Adam is implemented as Adaptive Gradient Update
- No GPU acceleration


** Archtecture and Implement Techniques

*** Class Archtecture
[[file:class.png]]

=NeuralNet= class has NonOutputLayer and =OutputLayer=. Theses layer
classes are stored with ~std::tuple<...>~ and managed by static polymorphism.


=NonOutputLayer= class has =Layer= class and =WeightBias= class. Since the
back propagation of weights and biases require activated values as
well as propagated gradient from the next layer, These 2 classes are
managed together as =NonOutputLayer= class.

*** Static Polymorphism
In usual dynamic polymorphism, pointers to classes derived from a
interface class are managed with unified way. This can be achived by
virtual function, whose address is recorded in =vtable= in class
instances and refered every time the function is called. This dynamic
runtime method cannot be optimized by any compiler.

By using =std::tuple= (from C++11), parameter pack (from C++11),
generic labmda (from C++14), folding expression (from C++17), and so
on, different classes with same name members can be treated like
dynamic polymorphism.

**** Apply function for all values in ~std::tuple<...>~
Since values in ~std::tuple~ have different types, we cannot access
them with access operator ~[]~.

Here we achive with ~std::apply~ (from C++17), folding expression
(C++17) and generic lambda (from C++14).  There are some examples from
[[file:DL_core.hh]].


Feed forward with operator =>>=.
#+BEGIN_SRC C++
friend auto operator>>(layer_type input,NeuralNet<Layers...>& nn){
  return std::apply([&](auto&&...l){ return (input >> ... >> l); },nn.layers);
}
#+END_SRC

Back propagete with operator =<<=.
#+BEGIN_SRC C++
auto operator<<(layer_type real){
  std::apply([&](auto&&...l){ (l << ... << real); },layers);
}
#+END_SRC

Call the =update= member function and sum up the return values.
#+BEGIN_SRC C++
void Update(value_type eps,value_type L1,value_type L2){
  std::apply([=](auto&...l){ (... + l.update(eps,L1,L2)); },layers);
}
#+END_SRC

The =std::apply= passes the values in =std::tuple= to function as
arguments. Even though type and size of values are unknown, generic
lambda with variadic template parameter (=[](auto...v){ }=) can take
and all the values packed in the parameter. The packed parameters are
unpacked with the operator =...= like followings:

1. comma separated in function, constructor, and template argument
   - =f(v...)= -> =f(v1,v2,v3,...)=.
2. Sequential binary operation (fold expression)
   - =(... + v)= -> =(((v1 + v2) + v3) + ...)=
   - =(v + ...)= -> =(... + (v3 + (v4 + v5)))=
   - =(init + ... + v)= -> =((((init + v1) + v2) + v3) + ...)=
   - =(v + ... + init)= -> =(... + (v3 + (v4 + (v5 + init))))=



*** TMP (template meta-programming) with SFINAE (substitution failuer is not an error)
C# generics must run with all the possible type, however, C++ template
does not have to. In C++, when a template is failed to instantiate,
the template is simply removed overloaded function set without error.

The ~type_traits~ header (from C++11) has many useful template classes
such as =std::enable_if=, =std::common_type=, =std::is_same= and so
on. (=XXX_t= classes and =XXX_v= classes are helper classes with are
same with =XXX::type= and =XXX::value= respectively. (from C++14))


Here is an exapmle from [[file:DL_core.hh]].

#+BEGIN_SRC C++
template<typename Adaptive,
	 std::enable_if_t<!std::is_same_v<std::remove_reference_t<Adaptive>,
					  value_type>,std::nullptr_t> = nullptr>
auto Update(Adaptive& a,value_type L1,value_type L2){
  return ymd::zip_for_each([=](auto&& l,auto&& a_){ return l.update(a_,L1,L2); },
			   layers,a);
}

void Update(value_type eps,value_type L1,value_type L2){
  std::apply([=](auto&...l){ (... + l.update(eps,L1,L2)); },layers);
}
#+END_SRC

Because of back compatibility, when adaptive update is implemented,
the original argument =value_type eps= must be accepted.

When you call =DL.Update(0.01,0.0,0.0)=, the upper candidate template
fails to instantiate because the deduced second template parameter,
=std::enable_if_t<false>= is not implemented (Only
=std::enable_if_t<true>= is implemented).

**** =std::enable_if<bool,std::nullptr_t> = nullptr= idiom (for expert)
The second template parameter of the =std::enable_if= is
=std::enable_if::type= and the type of =std::enable_if::value=. We set
=std::nullptr_t=, which is the type of =nullptr=, which prevent users
to set some temlate parameter without intention. (We assume there are
very rare case that users pass =nullptr= to template.)

There is a similar idiom named enabler idiom as follows:
#+BEGIN_SRC C++
extern void* enabler;

template<typename T,
	 typename std::enable_if<std::is_integral<T>::value>::type*& = enabler>
void f(T t){ }
#+END_SRC


** Test Environment
- Mac OS X 10.13.6
- g++ (MacPorts gcc8 8.2.0_0) 8.2.0
- g++ -O3 -march=native -Wa,-q -fdiagnostics-color=auto -std=c++2a
